{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "\n",
    "sc = np.array(sio.loadmat('data/raw/sCall_yeo.mat')['sCall'])\n",
    "fc = np.array(sio.loadmat('data/raw/fCall_yeo.mat')['fCall'])\n",
    "np.set_printoptions(suppress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9505\n",
      "950000\n"
     ]
    }
   ],
   "source": [
    "print(sc[np.where(sc == 0.0)].size)\n",
    "print(sc.size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100, 95)\n",
      "(100, 100, 95)\n",
      "[[1.         0.46285875 0.33626531 ... 0.21197098 0.19487723 0.09873838]\n",
      " [0.46285875 1.         0.49560592 ... 0.38396022 0.20020912 0.04987621]\n",
      " [0.33626531 0.49560592 1.         ... 0.31682414 0.15361462 0.1290186 ]\n",
      " ...\n",
      " [0.21197098 0.38396022 0.31682414 ... 1.         0.52977315 0.51036752]\n",
      " [0.19487723 0.20020912 0.15361462 ... 0.52977315 1.         0.82157261]\n",
      " [0.09873838 0.04987621 0.1290186  ... 0.51036752 0.82157261 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(sc.shape)\n",
    "print(fc.shape)\n",
    "print(fc[:, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95, 100, 100)\n"
     ]
    }
   ],
   "source": [
    "new_fc = np.transpose(fc, (2, 0, 1))  # convert to inductive dataset\n",
    "print(new_fc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[39m.\u001b[39meye(\u001b[39m100\u001b[39m)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mdtype\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.eye(100).float().dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from scipy.io import loadmat\n",
    "\n",
    "\n",
    "class MyDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(MyDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['sCall_yeo.mat', 'fCall_yeo.mat']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['processed_data.pt']\n",
    "\n",
    "    def download(self):\n",
    "        # Not required since the file is provided\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # Load the SC and FC data\n",
    "        sCall = loadmat(self.raw_paths[0])['sCall']  # shape (100, 100, 95)\n",
    "        fCall = loadmat(self.raw_paths[1])['fCall']  # shape (100, 100, 95)\n",
    "        fc_matrix = np.transpose(fCall, (2, 0, 1))  # convert to inductive dataset\n",
    "\n",
    "        \n",
    "        # Process each graph\n",
    "        data_list = []\n",
    "        num_nodes = sCall.shape[0]\n",
    "        for i in range(sCall.shape[-1]):\n",
    "            sc_matrix = torch.from_numpy(sCall[:, :, i])\n",
    "            # edge_index = torch.nonzero(sc_matrix, as_tuple=False).t()\n",
    "            edge_index = torch.tensor([(i, j) for i in range(num_nodes) for j in range(\n",
    "                num_nodes) if i != j], dtype=torch.long).t().contiguous()\n",
    "            edge_weight = sc_matrix[edge_index[0], edge_index[1]].unsqueeze(-1)\n",
    "            x = torch.eye(100)  # node features\n",
    "\n",
    "            # Convert fc matrix to a tensor and reshape it to a column vector\n",
    "            y = fc_matrix[i, :, :].reshape(-1, 1) # target edge weights\n",
    "            data_list.append(Data(x=x, edge_index=edge_index, edge_attr=edge_weight, y=y))\n",
    "\n",
    "        # Save the processed data\n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "\n",
    "class GCNOuterProductModel(nn.Module):\n",
    "    def __init__(self, num_nodes):\n",
    "        super(GCNOuterProductModel, self).__init__()\n",
    "        self.conv1 = GCNConv(100, 32)\n",
    "        self.num_nodes = num_nodes\n",
    "        # self.linear1 = nn.Linear(32 * num_nodes, num_nodes * num_nodes)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = torch.matmul(x, x.transpose(1, 0))\n",
    "        x = F.relu(x)\n",
    "        x = self.tanh(x)\n",
    "        return x.view(-1, self.num_nodes * self.num_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[800, 100], edge_index=[2, 79200], edge_attr=[79200, 1], y=[8], batch=[800], ptr=[9])\n",
      "(1, 10000)\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "for data in dataloader:\n",
    "    print(data)\n",
    "    y = np.array(y)\n",
    "    print((y.shape))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output torch.Size([1, 10000])\n",
      "Shape of target torch.Size([1, 10000])\n",
      "dtype of output torch.float32\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Create the dataset\n",
    "dataset = MyDataset('data/')\n",
    "\n",
    "# Create the dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Create the model and optimizer\n",
    "model = GCNOuterProductModel(num_nodes=100)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    loss = 0\n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        x, edge_index, edge_attr, y = data.x, data.edge_index, data.edge_attr, data.y\n",
    "        y = torch.tensor(y).squeeze(2)\n",
    "        pred = model(x, data.edge_index)\n",
    "        print(\"Shape of output\", pred.shape)\n",
    "        print(\"Shape of target\", y.shape)\n",
    "\n",
    "        print(\"dtype of output\", pred.dtype)\n",
    "        print(type(pred))\n",
    "        break\n",
    "    break\n",
    "        # curr_loss = F.mse_loss(pred.float(), y.float())\n",
    "        # loss += curr_loss\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "    # print('Epoch {}: Loss = {}'.format(epoch+1, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[100, 100], edge_index=[2, 9900], edge_attr=[9900, 1], y=[1], batch=[100], ptr=[2])\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (2360902333.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[220], line 10\u001b[0;36m\u001b[0m\n\u001b[0;31m    for i, data in dataset\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "model = GCNOuterProductModel(num_nodes=100)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = MSELoss()\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    for i, data in dataset\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = criterion(out, data.y[])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([3, 3])\n",
      "torch.Size([3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 5])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vector x vector\n",
    "tensor1 = torch.randn(3, 4)\n",
    "tensor2 = torch.randn(4, 3)\n",
    "print(tensor2.transpose(1, 0).shape)\n",
    "print(torch.matmul(tensor1, tensor2).size())\n",
    "# matrix x vector\n",
    "tensor1 = torch.randn(3, 4)\n",
    "tensor2 = torch.randn(4)\n",
    "print(torch.matmul(tensor1, tensor2).size())\n",
    "# batched matrix x broadcasted vector\n",
    "tensor1 = torch.randn(10, 3, 4)\n",
    "tensor2 = torch.randn(4)\n",
    "torch.matmul(tensor1, tensor2).size()\n",
    "# batched matrix x batched matrix\n",
    "tensor1 = torch.randn(10, 3, 4)\n",
    "tensor2 = torch.randn(10, 4, 5)\n",
    "torch.matmul(tensor1, tensor2).size()\n",
    "# batched matrix x broadcasted matrix\n",
    "tensor1 = torch.randn(10, 3, 4)\n",
    "tensor2 = torch.randn(4, 5)\n",
    "torch.matmul(tensor1, tensor2).size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "### another intresting model\n",
    "class GCNOuterProductModel(nn.Module):\n",
    "    def __init__(self, num_nodes):\n",
    "        super(GCNOuterProductModel, self).__init__()\n",
    "        self.conv1 = GCNConv(16, 32)\n",
    "        self.num_nodes = num_nodes\n",
    "        self.linear1 = nn.Linear(32 * num_nodes, num_nodes * num_nodes)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = x.view(-1, 32 * self.num_nodes)\n",
    "        x = self.linear1(x)\n",
    "        x = self.tanh(x)\n",
    "        return x.view(-1, self.num_nodes, self.num_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# subba_data1 = (sio.loadmat('new_data/fc_and_sc_sets.mat'))\n",
    "# subba_data2 = (sio.loadmat('new_data/desikan_roi_zhengwu.mat'))\n",
    "# subba_data3 = (sio.loadmat('new_data/destrieux_roi_zhengwu.mat'))\n",
    "\n",
    "fc_data_file = (sio.loadmat('new_data/correlations_desikan_old.mat'))\n",
    "sc_data_file = (sio.loadmat('new_data/scs_desikan.mat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'atlas', 'scs', 'subcortical_first', 'subject_list'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'ChosenROI_cortical', 'ChosenROI_subcortical', 'atlas', 'fcs', 'missing_data_rel2scs_patient_ids', 'subject_list'])\n",
      "(87, 87, 1065)\n",
      "(87, 87, 1058)\n"
     ]
    }
   ],
   "source": [
    "print(sc_data_file.keys())\n",
    "print(fc_data_file.keys())\n",
    "print(sc_data_file['scs'].shape)\n",
    "print(fc_data_file['fcs'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc_subject = sc_data_file['subject_list']\n",
    "sc_data = sc_data_file['scs']\n",
    "sc_matrix_t = np.transpose(sc_data, (2, 0, 1))\n",
    "type(sc_subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subject2matrix(subject_arr, mapping_arr):\n",
    "    \"\"\"\n",
    "    subject_arr: array of subjects (size: n_subjects x 1)\n",
    "    mapping_arr: array of matrices (size: n_subjects x n_nodes x n_nodes)\n",
    "    \"\"\"\n",
    "    my_dict = {}\n",
    "    for i in range(sc_subject.shape[0]):\n",
    "        my_dict[sc_subject[i, 0]] = sc_matrix_t[i]\n",
    "    return my_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1065\n",
      "1058\n"
     ]
    }
   ],
   "source": [
    "def subject2matrix(subject_arr, mapping_arr):\n",
    "    \"\"\"\n",
    "    subject_arr: array of subjects (size: n_subjects x 1)\n",
    "    mapping_arr: array of matrices (size: n_subjects x n_nodes x n_nodes)\n",
    "    \"\"\"\n",
    "    my_dict = {}\n",
    "    for i in range(subject_arr.shape[0]):\n",
    "        my_dict[subject_arr[i, 0]] = mapping_arr[i]\n",
    "    return my_dict\n",
    "\n",
    "sc_subject = sc_data_file['subject_list']\n",
    "sc_data = sc_data_file['scs']\n",
    "sc_matrix_t = np.transpose(sc_data, (2, 0, 1))\n",
    "sc_dict = subject2matrix(sc_subject, sc_matrix_t)\n",
    "print(len(sc_dict))\n",
    "\n",
    "fc_subject = fc_data_file['subject_list']\n",
    "fc_data = fc_data_file['fcs']\n",
    "fc_matrix_t = np.transpose(fc_data, (2, 0, 1))\n",
    "fc_dict = subject2matrix(fc_subject, fc_matrix_t)\n",
    "print(len(fc_dict))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1058"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paired_dict = {}\n",
    "for key in fc_dict.keys():\n",
    "    if key in sc_dict:\n",
    "        paired_dict[key] = (sc_dict[key], fc_dict[key])\n",
    "\n",
    "### create numpy array for sc and fc matrix\n",
    "### transpose them to (n_nodes, n_nodes, n_subjects)\n",
    "### save them as .mat file\n",
    "len(paired_dict)\n",
    "# print(paired_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1058\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(87, 87)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals = list(paired_dict.values())\n",
    "print(len(vals))\n",
    "np.array(vals)[1][1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 3\n",
      "shape: (3, 2)\n",
      "[1 3 5]\n",
      "[2 4 6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# example list of tuples\n",
    "list_of_tuples = [(1, 2), (3, 4), (5, 6)]\n",
    "print(\"size:\", len(list_of_tuples))\n",
    "\n",
    "# convert to numpy array\n",
    "arr = np.array(list_of_tuples)\n",
    "print(\"shape:\", arr.shape)\n",
    "\n",
    "# get separate arrays\n",
    "arr1 = arr[:, 0]\n",
    "arr2 = arr[:, 1]\n",
    "\n",
    "# print arrays\n",
    "print(arr1)\n",
    "print(arr2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100206 (87, 87) (87, 87)\n"
     ]
    }
   ],
   "source": [
    "for key, val in paired_dict.items():\n",
    "    print(key, val[0].shape, val[1].shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1058"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.intersect1d(sub_list_sc, sub_list_fc).size\n",
    "# len(comman_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# lstm = nn.LSTM(3, 3)  # Input dim is 3, output dim is 3\n",
    "# inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5\n",
    "\n",
    "# # initialize the hidden state.\n",
    "# hidden = (torch.randn(1, 1, 3),\n",
    "#           torch.randn(1, 1, 3))\n",
    "# for i in inputs:\n",
    "#     # Step through the sequence one element at a time.\n",
    "#     # after each step, hidden contains the hidden state.\n",
    "#     out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "\n",
    "# # alternatively, we can do the entire sequence all at once.\n",
    "# # the first value returned by LSTM is all of the hidden states throughout\n",
    "# # the sequence. the second is just the most recent hidden state\n",
    "# # (compare the last slice of \"out\" with \"hidden\" below, they are the same)\n",
    "# # The reason for this is that:\n",
    "# # \"out\" will give you access to all hidden states in the sequence\n",
    "# # \"hidden\" will allow you to continue the sequence and backpropagate,\n",
    "# # by passing it as an argument  to the lstm at a later time\n",
    "# # Add the extra 2nd dimension\n",
    "# inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "# hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # clean out hidden state\n",
    "# out, hidden = lstm(inputs, hidden)\n",
    "# print(out)\n",
    "# print(hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chirag-data-4OjJJ-VI-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a6462f1c47db2049d3ab93c40bcdadfa5d4d47f8cdfda7f59b4c947c31e4253"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
